{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pydicom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pydicom.dataset import Dataset, FileDataset\n",
    "from pydicom.uid import generate_uid\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 450 DICOM images.\n"
     ]
    }
   ],
   "source": [
    "def load_dicom_images(dicom_dir, image_size=(128, 128)):\n",
    "   \n",
    "    images = []\n",
    "    for filename in os.listdir(dicom_dir):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            filepath = os.path.join(dicom_dir, filename)\n",
    "            dicom = pydicom.dcmread(filepath)\n",
    "            image = dicom.pixel_array\n",
    "            image = cv2.resize(image, image_size)\n",
    "            image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "            images.append(image)\n",
    "\n",
    "    images = np.array(images)\n",
    "    images = np.expand_dims(images, axis=-1)\n",
    "    return images\n",
    "\n",
    "# Load images\n",
    "dicom_dir = '/home/matrix/Downloads/DIcom gans/Data/raw'\n",
    "images = load_dicom_images(dicom_dir)\n",
    "print(f\"Loaded {len(images)} DICOM images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_shape=(100,)):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128 * 32 * 32, input_dim=input_shape[0]),\n",
    "        tf.keras.layers.Reshape((32, 32, 128)),\n",
    "        tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2D(1, (7, 7), activation='tanh', padding='same')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_discriminator(input_shape=(128, 128, 1)):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), padding='same', input_shape=input_shape),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Build GAN\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = tf.keras.layers.Input(shape=(100,))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "    gan = tf.keras.Model(gan_input, gan_output)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matrix/.local/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/matrix/.local/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "# Compile models\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dicom_image(image, filepath, original_dicom):\n",
    "    \"\"\"\n",
    "    Save a single image as a DICOM file, preserving metadata from the original DICOM.\n",
    "    \"\"\"\n",
    "    ds = original_dicom.copy()\n",
    "    \n",
    "    # Update necessary attributes\n",
    "    ds.SOPInstanceUID = generate_uid()\n",
    "    ds.file_meta.MediaStorageSOPInstanceUID = ds.SOPInstanceUID\n",
    "    ds.PixelData = (image * 255).astype(np.uint8).tobytes()\n",
    "    ds.Rows, ds.Columns = image.shape\n",
    "    \n",
    "    # Save the DICOM file\n",
    "    ds.save_as(filepath)\n",
    "    print(f\"DICOM file saved: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile models\n",
    "# discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# gan.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# real_labels = np.ones((batch_size, 1))\n",
    "# fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     # Train discriminator\n",
    "#     idx = np.random.randint(0, images.shape[0], batch_size)\n",
    "#     real_images = images[idx]\n",
    "#     noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "#     fake_images = generator.predict(noise)\n",
    "    \n",
    "#     d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "#     d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "    \n",
    "#     # Train generator\n",
    "#     noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "#     g_loss = gan.train_on_batch(noise, real_labels)\n",
    "    \n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f\"{epoch} [D loss: {0.5 * np.add(d_loss_real, d_loss_fake)}] [G loss: {g_loss}]\")\n",
    "#         # Optionally save images and model checkpoints here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_as_dicom(generated_images, epoch, original_dicom, save_dir='outputs/generated_dicom'):\n",
    "    \"\"\"\n",
    "    Save generated images as DICOM files.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for i, img in enumerate(generated_images):\n",
    "        filepath = os.path.join(save_dir, f\"epoch{epoch}_img{i}.dcm\")\n",
    "        save_dicom_image(img.squeeze(), filepath, original_dicom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 693ms/step\n",
      "Epoch 1/1000, Time: 8.05s, D loss: 0.6887, G loss: 0.6897\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img0.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img1.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img2.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img3.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img4.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img5.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img6.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img7.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img8.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img9.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img10.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img11.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img12.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img13.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img14.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img15.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img16.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img17.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img18.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img19.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img20.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img21.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img22.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img23.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img24.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img25.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img26.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img27.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img28.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img29.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img30.dcm\n",
      "DICOM file saved: outputs/generated_dicom/epoch0_img31.dcm\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 640ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584ms/step\n",
      "Epoch 11/1000, Time: 6.92s, D loss: 0.6954, G loss: 0.6960\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 682ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725ms/step\n",
      "Epoch 21/1000, Time: 15.51s, D loss: 0.7019, G loss: 0.7025\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 754ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 1000  # Increase the number of epochs for better results\n",
    "save_interval = 100  # Save generated images every 100 epochs\n",
    "\n",
    "# Get a sample original DICOM for metadata\n",
    "sample_dicom = pydicom.dcmread(os.path.join(dicom_dir, os.listdir(dicom_dir)[0]))\n",
    "\n",
    "real_labels = np.ones((batch_size, 1))\n",
    "fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train discriminator\n",
    "    idx = np.random.randint(0, images.shape[0], batch_size)\n",
    "    real_images = images[idx]\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    fake_images = generator.predict(noise)\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "\n",
    "    # Train generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    g_loss = gan.train_on_batch(noise, real_labels)\n",
    "\n",
    "    # End of training step\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Time: {epoch_time:.2f}s, \"\n",
    "              f\"D loss: {0.5 * (d_loss_real[0] + d_loss_fake[0]):.4f}, \"\n",
    "              f\"G loss: {g_loss[0]:.4f}\")\n",
    "\n",
    "    # Save generated images as DICOM\n",
    "    if epoch % save_interval == 0:\n",
    "        save_images_as_dicom(fake_images, epoch, sample_dicom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this after the training loop\n",
    "\n",
    "num_to_generate = 450  # Set this to how many new images you want\n",
    "noise = np.random.normal(0, 1, (num_to_generate, 100))\n",
    "generated_images = generator.predict(noise)\n",
    "save_images_as_dicom(generated_images, \"final_batch\", sample_dicom)\n",
    "\n",
    "print(f\"Generated {num_to_generate} new DICOM images.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
